---
title: "Fanfiction Analysis"
author: "Johannes Burgers"
date: "1/20/2022"
output: rmdformats::html_clean
knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding = encoding, output_file = file.path(dirname(inputFile), 'index.html')) })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message=FALSE, cache.lazy = FALSE)
```


## libraries

The following analysis makes use of the `tidyverse` suite of tools, along with help from `quanteda` and `qdap` for tokenization and cleaning.

```{r load_libraries, message=FALSE, warning=FALSE}

library(tidytext)
library(tidyverse)
library(rmdformats)
library(htmlTable)
library(quanteda)
library(quanteda.textstats)
library(qdap)
library(scales)
```


```{r set_date}
#set data variable for versioning
date <- as.character(Sys.Date())
```


```{r get_files}
#get files

fanfiction_df <- read_csv("SomeTextLong.csv")
fanfiction_names_df <- read_csv("SomeNameLong.csv")
lor_all_files <- read_csv("TextsAboutLOR.csv")
```
```{r metadata}

metadata <- fanfiction_names_df %>% 
            left_join((fanfiction_df %>% select(!(Content))))

write_csv(metadata, "metadata.csv")
```

## Corpus Overview

The *Legolas* corpus is a subset of an extremely large corpus. For the sake of text processing, all non-English texts have been removed from the *Legolas* corpus. That said, on occasion there may be non-English words interspersed within these texts.


```{r overview}
total_words <- NULL

lor_all_words <- lor_all_files %>% 
                  summarise(total_words = sum(Word_count)) %>% 
                  mutate(corpus = "Lord of the Rings")

legolas_all_words <- fanfiction_names_df %>% 
  summarise(total_words = sum(Word_count)) %>% 
  mutate(corpus = "Legolas All Languages")

legolas_all_words_english <- fanfiction_names_df %>% 
  filter(Language == "English") %>% 
  summarise(total_words = sum(Word_count)) %>% 
  mutate(corpus = "Legolas English")

total_words <- total_words %>% 
               bind_rows(lor_all_words) %>% 
                bind_rows(legolas_all_words) %>% 
               bind_rows(legolas_all_words_english)


```


```{r overview_plot}
total_words %>% 
  ggplot(aes(x=reorder(corpus, total_words), y = total_words, fill = corpus))+
  geom_bar(stat="identity")+
           labs(title = "Total Words by Corpus",
       x = "Corpus on AO3",
       y = "Total Words in Millions",
       fill = "Author")+
  scale_y_continuous(labels = label_number(suffix = " M", scale = 1e-6))

```

## Generate descriptive statistics

First, generate an overview of the syntactical and lexical complexity. This can be done with the help of the `quantdata` package [see here for reference](https://towardsdatascience.com/linguistic-complexity-measures-for-text-nlp-e4bf664bd660).

The following series of functions take the `fanfiction_df` and turn it into a corpus object using only the `ID` and `Content` as variables. The remainder of the `df` is left aside for later joining.

```{r English_only, echo=TRUE}
#get list of English names
fanfiction_not_english <- fanfiction_names_df %>% 
                      filter(Language != "English") %>% 
                      select(ID)

```

```{r generate_small_df, echo=TRUE}
fanfiction_df_small <-  fanfiction_df %>% 
                       distinct(ID, Content) %>% 
                        anti_join(fanfiction_not_english)
```

Convert the `tibble` to a corpus object.

```{r convert_corpus_object, echo=TRUE, cache=TRUE}
# Creating a corpus
mycorpus <- corpus(fanfiction_df_small, docid_field = "ID", text_field = "Content")
```

Get basic descriptive statistics of text complexity. This information will need to be filtered out as some of the texts are clearly not accurately measured or have significant parts on in English.

```{r corpus_readability, echo=TRUE, cache=TRUE}
readability <- textstat_readability(mycorpus, c("meanSentenceLength","meanWordSyllables", "Flesch.Kincaid", "Flesch"), remove_hyphens = TRUE,
  min_sentence_length = 1, max_sentence_length = 10000,
  intermediate = FALSE) %>% 
  mutate(document = as.integer(document))
```

Join readability scores with author and work details.

```{r generate_author_score_df, echo=TRUE}
## Join the tibbles back by author ID
readability_text <- fanfiction_df %>% 
                    select(!(Content)) %>% 
                    left_join(readability, by = c("ID" = "document")) %>% 
                    left_join(fanfiction_names_df) %>% 
                    filter(Language == "English")

```

```{r write_readability}
write_csv(readability_text,paste("readability_per_text_",date,".csv", sep=""))
```


- Possible analysis, do a correlation analysis of reading score and likes/kudos or comments.

- Advanced analysis, do PCA of the different variables. 


## Tokenizing

Converting text elements to tokens. 

```{r tokenize_corpus, echo=TRUE, cache=TRUE, message=FALSE, eval=FALSE}
# Tokenisation
tok <- tokens(mycorpus, what = "word",
                   remove_punct = TRUE,
                   remove_symbols = TRUE,
                   remove_numbers = TRUE,
                   remove_url = TRUE,
                   remove_hyphens = FALSE,
                   verbose = TRUE, 
                   include_docvars = TRUE)
tok <- tokens_tolower(tok)
 
tok <- tokens_select(tok, stopwords("english"), selection = "remove", padding = FALSE)
```

Measuring lexical diversity through tokens

```{r lexical_diversity, echo=TRUE, cache=TRUE, eval=FALSE}
lexical_diversity <- dfm(tok) %>% 
  textstat_lexdiv(measure = "TTR")

```

## TTR Table

```{r lexical_diversity_table, echo=TRUE,cache=TRUE, eval = FALSE}
lexical_diversity <- lexical_diversity %>% 
                      mutate(document = as.numeric(document))

lexical_diversity_table <- fanfiction_df %>% 
                    select(!(Content)) %>% 
                    left_join(lexical_diversity, by = c("ID" = "document")) %>% 
                    left_join(fanfiction_names_df)

```
```{r write_lexical_diversity, eval = FALSE}
write_csv(lexical_diversity_table,paste("lexical_diversity_ttr_",date,".csv", sep=""))
```

```{r}
lexical_diversity_trimmed <- read_csv("lexical_diversity_ttr_2022-01-27.csv")

lexical_diversity_trimmed <- lexical_diversity_trimmed %>% 
                              filter(Language == "English") %>% 
                              filter(Word_count> 5000) %>% 
                              top_n(TTR, n = 10) %>% 
                              arrange(desc(TTR)) %>% 
                              select(Author, Title, TTR, Summary)

lexical_diversity_trimmed %>% 
  addHtmlTableStyle(col.rgroup = c("none", "#F5FBFF"),
                    pos.caption = "bottom") %>%
  htmlTable(caption = "Top 10 words based on Type-Token Ratio. Text's closer to 1 are more lexically rich.")


```



## NGRAM

Ngrams had to be created in chunks to account for the heavy memory usage.

```{r generate_ngrams, echo=TRUE, message=FALSE, warning=FALSE, cache=TRUE}
ngram_df <- NULL
for (i in 1:nrow(fanfiction_df_small)) {
         temp <- NULL
         temp <-  fanfiction_df_small %>% 
                   filter (row_number()==i) %>% 
                     group_by(ID) %>%      
                  unnest_tokens(bigram, Content, token = "ngrams", n = 2) %>% 
                  separate(bigram, c("word1", "word2"), sep = " ") %>% 
                  filter(!word1 %in% stop_words$word) %>%
                  filter(!word2 %in% stop_words$word) %>% 
                  mutate(bigram = paste(word1,word2, sep = " ")) %>% 
                  count(bigram, sort = TRUE) %>% 
                  top_n(n, n = 100)  
                  
        
         ngram_df <- ngram_df %>% 
                     bind_rows(temp)
          if (i%%250 == 0) {
      print(paste("Now processing text ",i," of", nrow(fanfiction_df_small), sep = ""))
            #added this to make sure R wasn't freezing. 
    }
}

```


```{r create_top_1000_bigrams, echo=TRUE, cache=FALSE}

ngram_total <- ngram_df %>% 
  group_by(bigram) %>% 
    summarise(total_bigrams = sum(n)) 

ngram_top_thousand <- ngram_total %>% 
                      ungroup() %>% 
                      arrange(desc(total_bigrams)) %>% 
                      top_n(n=1000)


ngram_top_ten_table <- ngram_top_thousand %>% 
                      top_n(n=50) %>% 
                addHtmlTableStyle(col.rgroup = c("none", "#F5FBFF"),
                    pos.caption = "bottom") %>%
  htmlTable(caption = "Top 50 ngrams")

ngram_top_ten_table
```

```{r}
write_csv(ngram_top_thousand, paste("ngram_top_thousand_",date,".csv", sep = ""))
```

## Distinct words

Measuring distinct words in a corpus is complex. Heaps' law indicates that the number of unique words is the inverse square of the total number of words. That is as texts get longer their unique words go down because there are only so many words in the English language. A better measure is ![tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). The code below does this for the entire corpus, but fetches a lot of false positives, because what it sees as "unique" are actually scraping errors. It's a good way to figure out what will need to be cleaned from the corpus.


```{r clean_book_words, echo=TRUE, cache=FALSE}
#Create a clean list of the count of each word in the corpus with contractions removed and possessives stripped, i.e. remove 's.

book_words_clean <- fanfiction_df_small %>% 
unnest_tokens(word, Content) 

book_words_clean_filtered <- book_words_clean %>% 
                            filter(!str_detect(word,"['’.]"))

book_words_punctuation <- book_words_clean %>% 
                            filter(str_detect(word,"['’`.]"))

book_words_punctuation <- book_words_punctuation %>% 
                          mutate(word = qdap::replace_contraction(word, sent.cap = FALSE))

book_words_punctuation_extended <- book_words_punctuation %>% 
                          mutate(word = str_remove_all(word,"['’.]")) %>% 
                          mutate(word = str_remove_all(word, " s")) %>% 
                          separate_rows(word, sep = " ")
```

```{r flush_RAM, eval=FALSE}
# These functions are necessary to reduce the load on memory.
rm(book_words_clean, book_words_punctuation)
gc()
```



```{r book_words_join, echo=TRUE, cache=TRUE}
book_words_final <- book_words_clean_filtered %>% 
                    bind_rows(book_words_punctuation_extended)
```


```{r second_RAM_flush, eval=FALSE}
rm(book_words_clean_filtered, book_words_punctuation_extended)
gc()

```


```{r book_words_final, echo=TRUE, cache=TRUE}

book_words_final <- book_words_final %>% 
                    count(ID, word, name = "nr_words", sort = TRUE) 
  
```

Get the total number of words by work.

```{r total_words, echo=TRUE, cache=TRUE}
total_words <- book_words_final %>% 
  group_by(ID) %>% 
  summarize(total = sum(nr_words))
```

Add the totals to the book_words tibble.

```{r book_words_and_totals, echo=TRUE, cache=TRUE, message=FALSE}
book_words_total <- left_join(book_words_final, total_words) 

```

Now calculate the tf_idf

```{r tf_Idf, echo=TRUE, cache=TRUE}
book_tf_idf <- book_words_total %>%
  bind_tf_idf(word, ID, nr_words)
```

### TF_IDF table

Thsi table needs to be joined back with the metadata to make some sense of it.

```{r create_TF_IDF_TABLE, message = FALSE}
book_tf_idf_table <- fanfiction_df %>% 
                    select(!(Content)) %>% 
                    left_join(book_tf_idf, by = c("ID" = "ID")) %>% 
                    left_join(fanfiction_names_df)
```



Get the top 3 distinct words for each book. This is still a lot of data to look through, but it might give an indication as to what is important about each work.


```{r distinct_words}
distinct_words <- book_tf_idf %>% 
  select(-total) %>%
  group_by(ID) %>% 
  arrange(desc(tf_idf)) %>% 
  slice_max(tf_idf, n = 3)
```

Join the metadata back to the distinct words table. 


```{r distinct_words_table}
distinct_words_table <-  fanfiction_df %>% 
                    select(!(Content)) %>% 
                    left_join(distinct_words, by = c("ID" = "ID")) %>% 
                    left_join(fanfiction_names_df)


 #distinct_words_table %>%  top_n(tf_idf, n=100) %>% 
  # addHtmlTableStyle(col.rgroup = c("none", "#F5FBFF"),
#                    pos.caption = "bottom") %>%
#  htmlTable(caption = "Top 100 unique words in the corpus by tf_idf rank and work")

```

```{r}
write_csv(distinct_words_table, paste("distinct_words_per_book",date,".csv", sep = ""))
```



Distinct words of the corpus.

```{r distinct_words_corpus, echo=TRUE, cache=TRUE}
distinct_words_total <- distinct_words %>% 
  ungroup() %>% 
  anti_join(stop_words) %>% 
  group_by(word) %>% 
  summarise(total_words = sum(nr_words)) %>% 
  arrange(desc(total_words))


distinct_words_total_table <- distinct_words_total %>% 
                              top_n(total_words, n= 50) 

distinct_words_total_table %>% 
  addHtmlTableStyle(col.rgroup = c("none", "#F5FBFF"),
                    pos.caption = "bottom") %>%
  htmlTable(header = c("Word", "Total Occurrences"),
            caption = "Top 50 unique words in the corpus by number or occurrences")
```


```{r write_out_data}
write_csv(distinct_words_total_table, paste("distinct_words_total",date,".csv", sep = ""))
```


```{r meta_data_unnest}

common_pairings <- metadata %>% 
                   group_by(ID) %>% 
                   separate_rows(Pairing, sep = ",") %>% 
                   mutate(Pairing = str_trim(Pairing)) %>% 
                   ungroup() %>% 
                   group_by(Pairing) %>% 
                   count()

common_pairings %>% 
 ggplot(aes(x=reorder(Pairing, n), y = n, fill = Pairing))+
  geom_bar(stat="identity")+
           labs(title = "Most Common Pairing",
       x = "Pairing",
       y = "Stories with this Pairing"
       )
```

```{r common_tags}
common_tags <- metadata %>% 
                   group_by(ID) %>% 
                   separate_rows(Tags, sep = ",") %>% 
                   mutate(Tags = str_trim(Tags)) %>% 
                   ungroup() %>% 
                   group_by(Tags) %>% 
                   count() %>% 
                   ungroup() %>% 
                   top_n(50) %>% 
                   arrange(desc(n))

htmlTable(common_tags)
```
```{r common_warnings}
common_warnings <- metadata %>% 
                   group_by(ID) %>% 
                   separate_rows(Warning, sep = ",") %>% 
                   mutate(Warning = str_trim(Warning)) %>% 
                   ungroup() %>% 
                   group_by(Warning) %>% 
                   count() %>% 
                   ungroup() %>% 
                   top_n(50) %>% 
                   arrange(desc(n))

common_warnings %>% 
 ggplot(aes(x=reorder(Warning, n), y = n, fill = Warning))+
  geom_bar(stat="identity")+
           labs(title = "Most Common Warning",
       x = "Warning",
       y = "Stories with this Warning"
       )
```
```{r top_comments}

comments_per_day <- metadata %>% 
                    mutate(days = Sys.Date() - as.Date(Date_published)) %>% 
                    mutate(across(starts_with("Num_"),~.x/as.numeric(days), .names = "{.col}_per_day"))


```


```{r}
write_csv(comments_per_day, "stats_by_days.csv")
```

